{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "main-model-code_BASIC_sigm.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0nKmzSwGhkfY",
        "outputId": "a15ff0b6-1acb-4e6f-8bd2-f574c6d0761b"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount = True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RkXtQzDAh8Zc"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from glob import glob\n",
        "import imageio\n",
        "import tensorflow as tf\n",
        "from matplotlib import pyplot\n",
        "import random\n",
        "import time\n",
        "import math\n",
        "\n",
        "from numpy import load\n",
        "from numpy import zeros\n",
        "from numpy import ones\n",
        "from numpy import savez_compressed\n",
        "from numpy.random import randint\n",
        "from keras import backend as K\n",
        "from keras.optimizers import Adam\n",
        "from keras.initializers import RandomNormal\n",
        "from keras.models import Model\n",
        "from keras.models import Input\n",
        "from keras.layers import Conv2D\n",
        "from keras.layers import Conv2DTranspose\n",
        "from keras.layers import LeakyReLU\n",
        "from keras.layers import Activation\n",
        "from keras.layers import Concatenate\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import BatchNormalization\n",
        "from keras.layers import LeakyReLU\n",
        "from matplotlib import pyplot"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xrynYeYYmZdk"
      },
      "source": [
        "### Hyperparameters ###\n",
        "\n",
        "#discriminator\n",
        "lr_dis = 0.0002 \n",
        "beta_1_dis = 0.5\n",
        "epsilon_dis = 1e-7\n",
        "loss_dis = 'binary_crossentropy'\n",
        "loss_weights_dis = [0.5]\n",
        "\n",
        "lrsched_dis = False                  #True --> gebruik lr scheduler, False --> niet\n",
        "lr_change_dis = 2                   #na hoeveel epochs wordt de lr aangepast\n",
        "drop_dis = 0.5                      #hoeveel neemt de lr af\n",
        "\n",
        "#gan\n",
        "lr_gan = 0.0002 \n",
        "beta_1_gan = 0.5\n",
        "epsilon_gan = 1e-7\n",
        "loss_gan = ['binary_crossentropy', 'mse']\n",
        "loss_weights_gan = [1,100]\n",
        "\n",
        "lrsched_gan = False                  #True --> gebruik lr scheduler, False --> niet\n",
        "lr_change_gan = 3                   #na hoeveel epochs wordt de lr aangepast\n",
        "drop_gan = 0.5                      #hoeveel neemt de lr af\n",
        "\n",
        "#train loop\n",
        "nr_epochs = 50\n",
        "batch_size= 4\n",
        "n_epochs_stop = 5\n",
        "\n",
        "# dropout in decoder generator\n",
        "\n",
        "\n",
        "#\n",
        "main_data_path = '/content/drive/Shareddrives/AI for Health/Data/'\n",
        "npz_dir = main_data_path+'train/'\n",
        "\n",
        "#CHANGE TO YOUR NAME\n",
        "model_name = 'baseline_sigm.h5'\n",
        "save_path = '/content/drive/Shareddrives/AI for Health/Models/Myrthe vd Berg/'+model_name[:-3]+'/'\n",
        "if os.path.exists(save_path) == False:\n",
        "  os.mkdir(save_path)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AvA1DJYWgAo3"
      },
      "source": [
        "**Data Inladen**\n",
        "\n",
        "Real samples validatie & train\n",
        "data inladen\n",
        "shapen naar [-1,1]\n",
        "reshapen met extra grayscale dimensie\n",
        "train/val split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oJSyvitVhJB9",
        "outputId": "2dbba96d-7f61-4b28-a1ab-c559cb91dcba"
      },
      "source": [
        "def load_real_samples(filename):\n",
        "    # load compressed arrays\n",
        "    data = load(filename)\n",
        "\n",
        "    # unpack arrays\n",
        "    X1, X2 = data['arr_0'], data['arr_1']\n",
        "\n",
        "    # scale from [0,255] to [0,1]\n",
        "    X1 = X1/255\n",
        "    X2 = X2/255\n",
        "    # reshape data (add 3rd grayscale dimension for 2d convolution layers)\n",
        "    X1 = X1.reshape(len(X1), 512, 512, 1)\n",
        "    X2 = X2.reshape(len(X1), 512, 512, 1)\n",
        "\n",
        "    #noise = np.random.normal(0, .1, X1.shape)\n",
        "    #X1 = X1 + noise\n",
        "    ind = list(range(0,len(X1)))\n",
        "    random.seed(1)\n",
        "    random.shuffle(ind)\n",
        "\n",
        "    train = [X1[ind[:792]], X2[ind[:792]]]\n",
        "    val = [X1[ind[792:]], X2[ind[792:]]]\n",
        "    return train, val, ind \n",
        "\n",
        "train_dataset, val_dataset, ind = load_real_samples(npz_dir+'train_dataset.npz')\n",
        "\n",
        "directory_train_files = os.listdir(main_data_path+'train/train_images/') \n",
        "trn_img_names = [file for file in directory_train_files if file.endswith('.png') or file.endswith('.jpg')]\n",
        "trn_img_names.sort()\n",
        "train_names = [trn_img_names[i] for i in ind[:792]]\n",
        "val_names = [trn_img_names[i] for i in ind[792:]]\n",
        "\n",
        "print(train_dataset[0].shape, train_dataset[1].shape, val_dataset[0].shape, val_dataset[1].shape)\n",
        "img_shape = train_dataset[0].shape[1:]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(792, 512, 512, 1) (792, 512, 512, 1) (98, 512, 512, 1) (98, 512, 512, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3sTXdxBgAib"
      },
      "source": [
        "**Discriminator**\n",
        "\n",
        "Functie voor discriminator\n",
        "\n",
        "Initialisatie discriminator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KgROKTDChJ43"
      },
      "source": [
        "# define the discriminator model\n",
        "def define_discriminator(image_shape=(512,512), lr=0.0002, beta_1=0.5, epsilon=1e-7, loss='binary_crossentropy', loss_weights=[0.5]):\n",
        "\t# weight initialization\n",
        "\tinit = RandomNormal(stddev=0.02)\n",
        "\t# source image input\n",
        "\tin_src_image = Input(shape=image_shape)\n",
        "\t# target image input\n",
        "\tin_target_image = Input(shape=image_shape)\n",
        "\t# concatenate images channel-wise\n",
        "\tmerged = Concatenate()([in_src_image, in_target_image])\n",
        "\t# C64\n",
        "\td = Conv2D(64, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(merged)\n",
        "\td = LeakyReLU(alpha=0.2)(d)\n",
        "\t# C128\n",
        "\td = Conv2D(128, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d)\n",
        "\td = BatchNormalization()(d)\n",
        "\td = LeakyReLU(alpha=0.2)(d)\n",
        "\t# C256\n",
        "\td = Conv2D(256, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d)\n",
        "\td = BatchNormalization()(d)\n",
        "\td = LeakyReLU(alpha=0.2)(d)\n",
        "\t# C512\n",
        "\td = Conv2D(512, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d)\n",
        "\td = BatchNormalization()(d)\n",
        "\td = LeakyReLU(alpha=0.2)(d)\n",
        "\t# second last output layer\n",
        "\td = Conv2D(512, (4,4), padding='same', kernel_initializer=init)(d)\n",
        "\td = BatchNormalization()(d)\n",
        "\td = LeakyReLU(alpha=0.2)(d)\n",
        "\t# patch output\n",
        "\td = Conv2D(1, (4,4), padding='same', kernel_initializer=init)(d)\n",
        "\tpatch_out = Activation('sigmoid')(d)\n",
        "\t# define model\n",
        "\tmodel = Model([in_src_image, in_target_image], patch_out)\n",
        "\t# compile model\n",
        "\topt = Adam(lr=lr, beta_1=beta_1, epsilon=epsilon)\n",
        "\tmodel.compile(loss=loss, optimizer=opt, loss_weights=loss_weights)\n",
        "\treturn model\n",
        "\n",
        "d_model = define_discriminator(img_shape, lr_dis, beta_1_dis, epsilon_dis, loss_dis, loss_weights_dis)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m1XOmRrTgAdi"
      },
      "source": [
        "**Generator**\n",
        "\n",
        "Code\n",
        "\n",
        "Initialisatie"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JJ9OfO7uhKWq"
      },
      "source": [
        "# define an encoder block\n",
        "def define_encoder_block(layer_in, n_filters, batchnorm=True):\n",
        "\t# weight initialization\n",
        "\tinit = RandomNormal(stddev=0.02)\n",
        "\t# add downsampling layer\n",
        "\tg = Conv2D(n_filters, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(layer_in)\n",
        "\t# conditionally add batch normalization\n",
        "\tif batchnorm:\n",
        "\t\tg = BatchNormalization()(g, training=True)\n",
        "\t# leaky relu activation\n",
        "\tg = LeakyReLU(alpha=0.2)(g)\n",
        "\treturn g\n",
        " \n",
        "# define a decoder block\n",
        "def decoder_block(layer_in, skip_in, n_filters, dropout=False):\n",
        "\t# weight initialization\n",
        "\tinit = RandomNormal(stddev=0.02)\n",
        "\t# add upsampling layer\n",
        "\tg = Conv2DTranspose(n_filters, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(layer_in)\n",
        "\t# add batch normalization\n",
        "\tg = BatchNormalization()(g, training=True)\n",
        "\t# conditionally add dropout\n",
        "\tif dropout:\n",
        "\t\tg = Dropout(0.5)(g, training=True)\n",
        "\t# merge with skip connection\n",
        "\tg = Concatenate()([g, skip_in])\n",
        "\t# leaky relu activation\n",
        "\tg = LeakyReLU(alpha=0.2)(g)\n",
        "\treturn g\n",
        " \n",
        "# define the standalone generator model\n",
        "def define_generator(image_shape=(512,512)):\n",
        "    # weight initialization\n",
        "    init = RandomNormal(stddev=0.02)\n",
        "    # image input\n",
        "    in_image = Input(shape=image_shape)\n",
        "    # encoder model\n",
        "    e1 = define_encoder_block(in_image, 64, batchnorm=False)\n",
        "    e2 = define_encoder_block(e1, 128)\n",
        "    e3 = define_encoder_block(e2, 256)\n",
        "    e4 = define_encoder_block(e3, 512)\n",
        "    e5 = define_encoder_block(e4, 512)\n",
        "    e6 = define_encoder_block(e5, 512)\n",
        "    e7 = define_encoder_block(e6, 512)\n",
        "    # bottleneck, no batch norm and relu\n",
        "    b = Conv2D(512, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(e7)\n",
        "    b = LeakyReLU(alpha=0.2)(b)\n",
        "    # decoder model\n",
        "    d1 = decoder_block(b, e7, 512)\n",
        "    d2 = decoder_block(d1, e6, 512)\n",
        "    d3 = decoder_block(d2, e5, 512)\n",
        "    d4 = decoder_block(d3, e4, 512, dropout=True)\n",
        "    d5 = decoder_block(d4, e3, 256, dropout=True)\n",
        "    d6 = decoder_block(d5, e2, 128, dropout=True)\n",
        "    d7 = decoder_block(d6, e1, 64, dropout=False)\n",
        "    # output\n",
        "    g = Conv2DTranspose(1, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d7)\n",
        "    out_image = Activation('sigmoid')(g)\n",
        "    # define model\n",
        "    model = Model(in_image, out_image)\n",
        "    return model\n",
        "\n",
        "g_model = define_generator(img_shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jUYzBiQygAYD"
      },
      "source": [
        "**Compile GAN**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jn9zpB88hK5z"
      },
      "source": [
        "# define the combined generator and discriminator model, for updating the generator\n",
        "def define_gan(g_model, d_model, image_shape=(512,512), lr=0.0002, beta_1=0.5, epsilon=1e-7, loss=['binary_crossentropy', 'mae'], loss_weights=[1,100]):\n",
        "\t# make weights in the discriminator not trainable\n",
        "\tfor layer in d_model.layers:\n",
        "\t\tif not isinstance(layer, BatchNormalization):\n",
        "\t\t\tlayer.trainable = False\n",
        "\t# define the source image\n",
        "\tin_src = Input(shape=image_shape)\n",
        "\t# connect the source image to the generator input\n",
        "\tgen_out = g_model(in_src)\n",
        "\t# connect the source input and generator output to the discriminator input\n",
        "\tdis_out = d_model([in_src, gen_out])\n",
        "\t# src image as input, generated image and classification output\n",
        "\tmodel = Model(in_src, [dis_out, gen_out])\n",
        "\t# compile model\n",
        "\topt = Adam(lr=lr, beta_1=beta_1, epsilon=epsilon)\n",
        "\tmodel.compile(loss=loss, optimizer=opt, loss_weights=loss_weights)\n",
        "\treturn model\n",
        "\n",
        "gan_model = define_gan(g_model, d_model, img_shape, lr_gan, beta_1_gan, epsilon_gan, loss_gan, loss_weights_gan)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zr8dmNbJgARz"
      },
      "source": [
        "**Fake & real samples**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SFbssTCGhLQ7"
      },
      "source": [
        "# generate a batch of images, returns images and targets\n",
        "def generate_fake_samples(g_model, samples, patch_shape):\n",
        "\t# generate fake instance\n",
        "\tX = g_model.predict(samples)\n",
        "\t# create 'fake' class labels (0)\n",
        "\ty = zeros((len(X), patch_shape, patch_shape, 1))\n",
        "\treturn X, y\n",
        "def generate_real_samples(dataset, n_samples, patch_shape):\n",
        "\t# unpack dataset\n",
        "\ttrainA, trainB = dataset\n",
        "\t# choose random instances\n",
        "\tix = randint(0, trainA.shape[0], n_samples)\n",
        "\t# retrieve selected images\n",
        "\tX1, X2 = trainA[ix], trainB[ix]\n",
        "\t# generate 'real' class labels (1)\n",
        "\ty = ones((n_samples, patch_shape, patch_shape, 1))\n",
        "\treturn [X1, X2], y "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ohrs23VwgALd"
      },
      "source": [
        "**Performance functies**\n",
        "\n",
        "functie maken voor:\n",
        "\n",
        "--> loss functie validatie + opslaan in lijst voor plot\n",
        "\n",
        "--> opslaan van generator model als validatie loss lager is\n",
        "\n",
        "--> early stopping (als validatie loss niet meer afneemt)\n",
        "\n",
        "ZS dorian:\n",
        "* Disc loss\n",
        "* Gen loss\n",
        "* 3 plots of example images with dice scores \n",
        "* Save of the model\n",
        "* The amount of epochs\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FQ7nCqaxQWx_"
      },
      "source": [
        "def dice(pred, true, k = 1):\n",
        "    intersection = np.sum(pred[true==k]) * 2.0\n",
        "    dice = intersection / (np.sum(pred) + np.sum(true))\n",
        "    return dice\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "izucTAW7hLuk"
      },
      "source": [
        "def summarize_val_performance(epoch, g_model, val_dataset, n_epochs_stop, model_name, save_path, mean_val_loss, std_val_loss):\n",
        "    #validatie loss bepalen\n",
        "    #Opslaan g_model --> model_name uit hyperparameter selectie\n",
        "    #Early stopping checken\n",
        "\n",
        "    val_losses = list()\n",
        "    best_loss = list()\n",
        "\n",
        "    #for plotting\n",
        "    #save_plot_ind = randint(0, len(val_dataset[0]), 3)\n",
        "    row = 0 \n",
        "    array = np.zeros((len(val_dataset[0]),512, 512,1))\n",
        "    ### Predict val set + Loss ###\n",
        "    for idx in range(len(val_dataset[0])):\n",
        "        loss = list()\n",
        "        src_image = val_dataset[0][idx]\n",
        "        src_image = np.expand_dims(src_image, 0)\n",
        "        gen_image = g_model.predict(src_image)\n",
        "        gt_image = val_dataset[1][idx]\n",
        "        # scale from [-1,1] to [0,1]\n",
        "        #src_image = (src_image + 1) / 2.0\n",
        "        gen_image = np.where(gen_image > 0.5, 1, 0)\n",
        "        #gt_image = (gt_image + 1) / 2.0\n",
        "\n",
        "        ### CALCULATE LOSS ###\n",
        "        loss = -dice(gen_image.reshape(512,512), gt_image.reshape(512,512))#np.mean(np.square(gt_image-gen_image))#\n",
        "        val_losses.append(loss)\n",
        "\n",
        "        #gen_image = np.where(gen_image > 0.5, 1, 0)\n",
        "        array[idx]=gen_image\n",
        "        ### CREATE FIGURE ###\n",
        "    val_array = np.array(val_losses)\n",
        "    plotlist = val_array.argsort()[-2:][::-1]\n",
        "    plotlist = list(plotlist)\n",
        "    plotlist.append(val_losses.index(np.min(val_losses)))\n",
        "    #if idx in save_plot_ind:\n",
        "    for idx in plotlist:\n",
        "      if row == 0:\n",
        "        fig, ax = plt.subplots(3,3, figsize=(17,17))\n",
        "        fig.subplots_adjust(hspace=0.5)\n",
        "\n",
        "      ax[row,0].imshow(val_dataset[0][idx].reshape(512,512), cmap='gray')\n",
        "      ax[row,0].axis('off')\n",
        "      ax[row,0].set_title(val_names[idx])\n",
        "\n",
        "      ax[row,1].imshow(array[idx].reshape(512,512), cmap='gray')\n",
        "      ax[row,1].axis('off')\n",
        "      ax[row,1].set_title('dice={:.3f}'.format(-val_losses[idx]))\n",
        "\n",
        "      ax[row,2].imshow(val_dataset[1][idx].reshape(512,512), cmap='gray')\n",
        "      ax[row,2].axis('off')\n",
        "      ax[row,2].set_title('True mask')\n",
        "      \n",
        "      row +=1\n",
        "    \n",
        "    ### SAVE FIGURE ###\n",
        "    filename1 = save_path+'plot_{}_ep_{}.png'.format(model_name, epoch)\n",
        "    pyplot.savefig(filename1)\n",
        "    plt.close()\n",
        "\n",
        "    mean_val_loss.append(np.mean(val_losses))\n",
        "    std_val_loss.append(np.std(val_losses))\n",
        "    return mean_val_loss, std_val_loss\n",
        "\n",
        "def summarize_train_performance(epoch, g_model, train_dataset, mean_train_loss, std_train_loss):\n",
        "    #validatie loss bepalen\n",
        "    #Opslaan g_model --> model_name uit hyperparameter selectie\n",
        "    #Early stopping checken\n",
        "\n",
        "    train_losses = list()\n",
        "\n",
        "    ### Predict train set + Loss ###\n",
        "    for idx in range(len(train_dataset[0])):\n",
        "        loss = list()\n",
        "        src_image = train_dataset[0][idx]\n",
        "        src_image = np.expand_dims(src_image, 0)\n",
        "        gen_image = g_model.predict(src_image)\n",
        "        gt_image = train_dataset[1][idx]\n",
        "        # scale from [-1,1] to [0,1]\n",
        "        #src_image = (src_image + 1) / 2.0\n",
        "        gen_image = np.where(gen_image > 0.5, 1, 0)\n",
        "        #gt_image = (gt_image + 1) / 2.0\n",
        "\n",
        "        ### CALCULATE LOSS ###\n",
        "        loss = -dice(gen_image.reshape(512,512), gt_image.reshape(512,512))#np.mean(np.square(gt_image-gen_image))#\n",
        "        train_losses.append(loss)\n",
        "    \n",
        "\n",
        "    mean_train_loss.append(np.mean(train_losses))\n",
        "    std_train_loss.append(np.std(train_losses))\n",
        "\n",
        "    return mean_train_loss, std_train_loss\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wrj6dxxygACm"
      },
      "source": [
        "**Training loop**\n",
        "\n",
        "idee:\n",
        "\n",
        "```\n",
        "for epoch in range(epochs)\n",
        "    for step in range(steps)\n",
        "        train discriminator met real samples\n",
        "        train discriminator met fake samples\n",
        "        update generator met 'define_gan' functie\n",
        "\n",
        "        --> d1_loss, d2_loss & g_loss opslaan\n",
        "    \n",
        "    validatie loss bepalen\n",
        "    opslaan generator model als loss lager is\n",
        "    early stopping checken\n",
        "\n",
        "```\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y24ozKZVhMJr"
      },
      "source": [
        "# train pix2pix models\n",
        "\n",
        "\n",
        "def train(d_model, g_model, gan_model, dataset, n_epochs, n_batch):\n",
        "\n",
        "    plot_d_loss1 = list()\n",
        "    plot_d_loss2 = list()\n",
        "    plot_g_loss = list()\n",
        "    mean_val_loss = list()\n",
        "    mean_train_loss = list()\n",
        "    std_val_loss = list()\n",
        "    std_train_loss = list()\n",
        "\n",
        "    # determine the output square shape of the discriminator\n",
        "    n_patch = d_model.output_shape[1]\n",
        "    # unpack dataset\n",
        "    trainA, trainB = dataset\n",
        "    # calculate the number of batches per training epoch\n",
        "    bat_per_epo = int(len(trainA) / n_batch)\n",
        "\n",
        "    # manually enumerate epochs\n",
        "    for epoch in range(n_epochs):\n",
        "        ########set learning rate with lr schedule################\n",
        "        if lrsched_dis == True and epoch % lr_change_dis == 0:\n",
        "            new_lr_dis = lr_dis * math.pow(drop_dis, math.floor((1+epoch)/lr_change_dis))\n",
        "            K.set_value(d_model.optimizer.learning_rate, new_lr_dis)\n",
        "            print('Lr dis changed to {}'.format(d_model.optimizer.learning_rate))\n",
        "\n",
        "        if lrsched_gan == True and epoch % lr_change_gan == 0:\n",
        "            new_lr_gan = lr_gan * math.pow(drop_gan, math.floor((1+epoch)/lr_change_gan))\n",
        "            K.set_value(gan_model.optimizer.learning_rate, new_lr_gan)\n",
        "            print('Lr gan changed to {}'.format(gan_model.optimizer.learning_rate))\n",
        "          \n",
        "        for i in range(bat_per_epo):\n",
        "          print(\"\\r Epoch {} progress: {} % \".format(epoch, np.round((i+1)/bat_per_epo*100,2)), end=\"\")\n",
        "          # select a batch of real samples\n",
        "          [X_realA, X_realB], y_real = generate_real_samples(dataset, n_batch, n_patch)\n",
        "          # generate a batch of fake samples\n",
        "          X_fakeB, y_fake = generate_fake_samples(g_model, X_realA, n_patch)\n",
        "          # update discriminator for real samples\n",
        "          d_loss1 = d_model.train_on_batch([X_realA, X_realB], y_real)\n",
        "          # update discriminator for generated samples\n",
        "          d_loss2 = d_model.train_on_batch([X_realA, X_fakeB], y_fake)\n",
        "          # update the generator\n",
        "          g_loss, _, _ = gan_model.train_on_batch(X_realA, [y_real, X_realB])\n",
        "          # summarize performance\n",
        "          \n",
        "          ########Losses opslaan################\n",
        "          plot_d_loss1.append(d_loss1) \n",
        "          plot_d_loss2.append(d_loss2)\n",
        "          plot_g_loss.append(g_loss)\n",
        "          \n",
        "          #print('>%d, d1[%.3f] d2[%.3f] g[%.3f]' % (i+1, d_loss1, d_loss2, g_loss))\n",
        "          \n",
        "          \n",
        "        # save losses in case model crashes\n",
        "        npzname = save_path+'loss_list.npz'\n",
        "        savez_compressed(npzname, plot_g_loss, plot_d_loss1, plot_d_loss2)\n",
        "        # check validatie loss, model opslaan, early stopping\n",
        "\n",
        "        mean_val_loss, std_val_loss = summarize_val_performance(epoch, g_model, val_dataset, n_epochs_stop, model_name, save_path, mean_val_loss, std_val_loss)\n",
        "        mean_train_loss, std_train_loss = summarize_train_performance(epoch, g_model, train_dataset, mean_train_loss, std_train_loss)\n",
        "        print('epoch: ', epoch, '\\ttrain loss: {:.4f} std: {:.4f}'.format(mean_train_loss[-1], std_train_loss[-1]), '\\tval loss: {:.4f} std: {:.4f}'.format(mean_val_loss[-1], std_val_loss[-1]))\n",
        "\n",
        "        ### SAVE BEST MODEL ###\n",
        "        #Keep track of best generalizing model\n",
        "        #First run to log model & val_loss\n",
        "        if epoch == 0:\n",
        "          best_model = g_model\n",
        "          best_loss = mean_val_loss[0]\n",
        "          best_model.save(save_path+model_name)\n",
        "        #Check if loss is lower than previous epoch, if so save model as best_model & save the val loss\n",
        "        if epoch > 0 and mean_val_loss[-1] < best_loss:\n",
        "          best_model = g_model\n",
        "          best_model.save(save_path+model_name)\n",
        "          best_loss = mean_val_loss[-1]\n",
        "          best_epoch = epoch\n",
        "          print('logged best model, val loss = ', best_loss)\n",
        "\n",
        "        npzname2 = save_path+'epoch_loss_list.npz'\n",
        "        savez_compressed(npzname2, mean_val_loss, mean_train_loss, std_val_loss, std_train_loss)\n",
        "\n",
        "        ### EARLY STOPPING ###\n",
        "        if epoch == 0:\n",
        "          epochs_no_improve = 0 \n",
        "        #for early stopping check if the loss is lower than the previous epoch, if yes, epochs_no_improve =0            \n",
        "        if epoch > 0 and (mean_val_loss[-1] < mean_val_loss[-2] and ((-mean_val_loss[-1]) - (-mean_val_loss[-2])) > 0.001):\n",
        "            epochs_no_improve = 0\n",
        "        #for early stopping check if the loss is lower than the previous epoch, if not, epochs_no_improve +=1            \n",
        "        if epoch > 0 and (mean_val_loss[-1] > mean_val_loss[-2] or ((-mean_val_loss[-1]) - (-mean_val_loss[-2])) < 0.001):\n",
        "          epochs_no_improve += 1     \n",
        "          print('Not improved count: ', epochs_no_improve)\n",
        "        #if epochs_no_improve is the count where we want the model to stop, break the epoch forloop\n",
        "        if epoch > 0 and epochs_no_improve == n_epochs_stop:\n",
        "          print('No more improvement, break')\n",
        "          break\n",
        "    return mean_val_loss, std_val_loss, mean_train_loss, std_train_loss, best_loss, best_epoch, plot_g_loss, plot_d_loss1, plot_d_loss2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tCcKW1H9Xlz6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PNnSyLenfzBD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b1fc2d6-dd69-4603-93cf-99f1a76e1705"
      },
      "source": [
        "with tf.device('/device:GPU:0'):\n",
        "  start_time = time.time()\n",
        "  mean_val_loss, std_val_loss, mean_train_loss, std_train_loss, best_val_loss, best_epoch, plot_g_loss, plot_d_loss1, plot_d_loss2 = train(d_model, g_model, gan_model, train_dataset, nr_epochs, batch_size)\n",
        "  print(\"--- %s minutes ---\" % ((time.time() - start_time)/60))\n",
        "  print('Epoch: {}, best val loss: {}'.format(best_epoch, best_val_loss))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Epoch 0 progress: 29.29 % "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Usa8mCSGqvHa"
      },
      "source": [
        "def plot_loss(g_loss, d1_loss, d2_loss):\n",
        "  # summarize history for loss\n",
        "  fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(12,12), sharex=True)\n",
        "  fig.subplots_adjust(hspace=0.1)\n",
        "\n",
        "  ax1.plot(g_loss, 'g')\n",
        "  ax1.set_ylabel('Generator loss')\n",
        "  ax1.set_xlim(0, len(g_loss)-1)\n",
        " \n",
        "\n",
        "  ax2.plot(d1_loss, 'r')\n",
        "  ax2.set_ylabel('Real images loss')\n",
        "  ax2.set_xlim(0, len(d1_loss)-1)\n",
        "\n",
        "\n",
        "  ax3.plot(d2_loss, 'b')\n",
        "  ax3.set_ylabel('Fake images loss')\n",
        "  ax3.set_xlim(0, len(d2_loss)-1)\n",
        " \n",
        " \n",
        "  plt.xlabel('steps')\n",
        "  filename2 = save_path+'losses_{}.png'.format(model_name)\n",
        "  pyplot.savefig(filename2)\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m6Kl7CjfYENj"
      },
      "source": [
        "def plot_avg_step_loss(g_loss, d1_loss, d2_loss, nr_epochs, batch_size):\n",
        "  # summarize history for loss\n",
        "  fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(12,12), sharex=True)\n",
        "  fig.subplots_adjust(hspace=0.1)\n",
        "\n",
        "  steps_per_epoch = int(1576 / batch_size)\n",
        "  avg_idx = [steps_per_epoch] * nr_epochs\n",
        "\n",
        "  g_loss_copy = list(g_loss[:])\n",
        "  g_loss_means = [sum(g_loss_copy.pop(0) for _ in range(i)) / i for i in avg_idx]\n",
        "  \n",
        "  d1_loss_copy = list(d1_loss[:])\n",
        "  d1_loss_means = [sum(d1_loss_copy.pop(0) for _ in range(i)) / i for i in avg_idx]\n",
        "\n",
        "  d2_loss_copy = list(d2_loss[:])\n",
        "  d2_loss_means = [sum(d2_loss_copy.pop(0) for _ in range(i)) / i for i in avg_idx]\n",
        "\n",
        "  ax1.plot(g_loss_means, 'g')\n",
        "  ax1.set_ylabel('Generator loss')\n",
        "  ax1.set_xlim(0, len(g_loss_means)-1)\n",
        " \n",
        "\n",
        "  ax2.plot(d1_loss_means, 'r')\n",
        "  ax2.set_ylabel('Real images loss')\n",
        "  ax2.set_xlim(0, len(d1_loss_means)-1)\n",
        "\n",
        "\n",
        "  ax3.plot(d2_loss_means, 'b')\n",
        "  ax3.set_ylabel('Fake images loss')\n",
        "  ax3.set_xlim(0, len(d2_loss_means)-1)\n",
        " \n",
        " \n",
        "  plt.xlabel('Epoch')\n",
        "  filename2 = save_path+'mean_losses_{}.png'.format(model_name)\n",
        "  pyplot.savefig(filename2)\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gY984K1faKlz"
      },
      "source": [
        "data = load(save_path+'loss_list.npz')\n",
        "\n",
        "# unpack arrays\n",
        "plot_g_loss, plot_d_loss1, plot_d_loss2 = data['arr_0'], data['arr_1'], data['arr_2']\n",
        "plot_avg_step_loss(plot_g_loss, plot_d_loss1, plot_d_loss2, 50, batch_size) # set to 14 because I interrupted model training after epoch 14 because of time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eBWILXhhIvA4"
      },
      "source": [
        "data = load(save_path+'loss_list.npz')\n",
        "\n",
        "# unpack arrays\n",
        "plot_g_loss, plot_d_loss1, plot_d_loss2 = data['arr_0'], data['arr_1'], data['arr_2']\n",
        "plot_loss(plot_g_loss, plot_d_loss1, plot_d_loss2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hCF4yFThJG8u"
      },
      "source": [
        "data = load(save_path+'epoch_loss_list.npz')\n",
        "\n",
        "# unpack arrays\n",
        "mean_val_loss, mean_train_loss = data['arr_0'], data['arr_1']\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(mean_val_loss, label='Validation loss')\n",
        "plt.plot(mean_train_loss, label='Training loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('-dice value')\n",
        "plt.legend()\n",
        "filename3 = save_path+'epoch_trainval_loss_{}.png'.format(model_name)\n",
        "pyplot.savefig(filename3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tFmPI1eRo8-8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}